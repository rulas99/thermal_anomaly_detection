{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thermal_anomalies_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14zgCudlqe4JJV6kcpPNWkbZ-jSIx9trr",
      "authorship_tag": "ABX9TyMS6ejWQia/tvb4uNSEq6zP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rulas99/thermal_anomaly_detection/blob/main/thermal_anomalies_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "alFBr4KgC-gb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models.segmentation\n",
        "import torch\n",
        "import torchvision.transforms as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 1e-5\n",
        "width = 505\n",
        "height = 509 # image width and height\n",
        "batchSize = 3"
      ],
      "metadata": {
        "id": "7N5s4KO5wC0h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True) # Load net\n",
        "\n",
        "Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 3 classes\n",
        "\n",
        "Net=Net.to(device)\n",
        "\n",
        "optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer"
      ],
      "metadata": {
        "id": "Lx6dEFuLwLkK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainFolder=\"LabPics/Simple/Train//\"\n",
        "\n",
        "ListImages=os.listdir(os.path.join(TrainFolder, \"Image\")) # Create list of images"
      ],
      "metadata": {
        "id": "PRKw-z48wAVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReadRandomImage(): # First lets load random image and  the corresponding annotation\n",
        "    idx=np.random.randint(0,len(ListImages)) # Select random image\n",
        "    Img=cv2.imread(os.path.join(TrainFolder, \"Image\", ListImages[idx]))[:,:,0:3]\n",
        "    Filled =  cv2.imread(os.path.join(TrainFolder, \"Semantic/16_Filled\", ListImages[idx].replace(\"jpg\",\"png\")),0)\n",
        "    Vessel =  cv2.imread(os.path.join(TrainFolder, \"Semantic/1_Vessel\", ListImages[idx].replace(\"jpg\",\"png\")),0)\n",
        "    AnnMap = np.zeros(Img.shape[0:2],np.float32)\n",
        "    if Vessel is not None:  AnnMap[ Vessel == 1 ] = 1\n",
        "    if Filled is not None:  AnnMap[ Filled  == 1 ] = 2\n",
        "    Img=transformImg(Img)\n",
        "    AnnMap=transformAnn(AnnMap)\n",
        "    return Img,AnnMap\n",
        "\n",
        "\n",
        "def LoadBatch(): # Load batch of images\n",
        "    images = torch.zeros([batchSize,3,height,width])\n",
        "    ann = torch.zeros([batchSize, height, width])\n",
        "    for i in range(batchSize):\n",
        "        images[i],ann[i]=ReadRandomImage()\n",
        "    return images, ann"
      ],
      "metadata": {
        "id": "yRdqSGrxwJMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for itr in range(10000): # Training loop\n",
        "   images,ann=LoadBatch() # Load taining batch\n",
        "   images=torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "   ann = torch.autograd.Variable(ann, requires_grad=False).to(device) # Load annotation\n",
        "   Pred=Net(images)['out'] # make prediction\n",
        "   Net.zero_grad()\n",
        "   criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
        "   Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss\n",
        "   Loss.backward() # Backpropogate loss\n",
        "   optimizer.step() # Apply gradient descent change to weight\n",
        "   seg = torch.argmax(Pred[0], 0).cpu().detach().numpy()  # Get  prediction classes\n",
        "   print(itr,\") Loss=\",Loss.data.cpu().numpy())\n",
        "   if itr % 1000 == 0: #Save model weight once every 60k steps permenant file\n",
        "        print(\"Saving Model\" +str(itr) + \".torch\")\n",
        "        torch.save(Net.state_dict(),   str(itr) + \".torch\")"
      ],
      "metadata": {
        "id": "prG5whv_wQtH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}