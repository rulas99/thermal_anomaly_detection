{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thermal_anomalies_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14zgCudlqe4JJV6kcpPNWkbZ-jSIx9trr",
      "authorship_tag": "ABX9TyNSlYqoYFpsntdM1vjG/M9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rulas99/thermal_anomaly_detection/blob/main/thermal_anomalies_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Segmentation [source](https://towardsdatascience.com/train-neural-net-for-semantic-segmentation-with-pytorch-in-50-lines-of-code-830c71a6544f)"
      ],
      "metadata": {
        "id": "aekdsIGqHvuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "alFBr4KgC-gb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models.segmentation\n",
        "import torch\n",
        "import torchvision.transforms as tf\n",
        "\n",
        "from pandas import read_csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 1e-5\n",
        "width = 500\n",
        "height = 500 # image width and height\n",
        "batchSize = 7"
      ],
      "metadata": {
        "id": "7N5s4KO5wC0h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True) # Load net\n",
        "\n",
        "Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 3 classes\n",
        "\n",
        "Net = Net.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # Set loss function"
      ],
      "metadata": {
        "id": "Lx6dEFuLwLkK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=read_csv(\"/content/drive/MyDrive/segment_sentinel2_hotspot/train.csv\")"
      ],
      "metadata": {
        "id": "PRKw-z48wAVu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)),tf.ToTensor(),])\n",
        "                         #tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "                         \n",
        "transformAnn=tf.Compose([tf.ToPILImage(),tf.Resize((height,width),tf.InterpolationMode.NEAREST),tf.ToTensor()])"
      ],
      "metadata": {
        "id": "cr6fGoMHAgsQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReadRandomImage(): # First lets load random image and  the corresponding annotation\n",
        "    idx=np.random.randint(0,len(train)) # Select random image\n",
        "    thermal = cv2.imread(train.ori_path.iloc[idx])\n",
        "    masked =  cv2.imread(train.label_path.iloc[idx],0)\n",
        "    AnnMap = np.zeros(thermal.shape[0:2],np.float32)\n",
        "    if masked is not None:  AnnMap[ masked == 255 ] = 1\n",
        "    Img=transformImg(thermal)\n",
        "    AnnMap=transformAnn(AnnMap)\n",
        "    return Img,AnnMap\n",
        "\n",
        "\n",
        "def LoadBatch(): # Load batch of images\n",
        "    images = torch.zeros([batchSize,3,height,width])\n",
        "    ann = torch.zeros([batchSize, height, width])\n",
        "    for i in range(batchSize):\n",
        "        images[i],ann[i]=ReadRandomImage()\n",
        "    return images, ann"
      ],
      "metadata": {
        "id": "yRdqSGrxwJMk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for itr in range(200): # Training loop\n",
        "   images,ann=LoadBatch() # Load taining batch\n",
        "   images=torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "   ann = torch.autograd.Variable(ann, requires_grad=False).to(device) # Load annotation\n",
        "   Pred=Net(images)['out'] # make prediction\n",
        "   optimizer.zero_grad()\n",
        "   Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss\n",
        "   Loss.backward() # Backpropogate loss\n",
        "   optimizer.step() # Apply gradient descent change to weight\n",
        "\n",
        "   if itr % 10 == 0: #Save model weight once every 100 steps permenant file\n",
        "        accG = [(torch.argmax(Pred[i], 0).cpu().detach().numpy() == ann[i].cpu().detach().numpy()).sum()/(width*height) for i in range(batchSize)]\n",
        "        accM = round(sum(accG)/batchSize,5)\n",
        "        loss = round(float(Loss.data.cpu()),5)\n",
        "        print(itr,f\") Loss = {loss} -- Accuracy = {accM}\")\n",
        "        print(f\"Saving model_{itr}.torch\")\n",
        "        torch.save(Net.state_dict(),   f'/content/drive/MyDrive/segment_sentinel2_hotspot/Models/model_{itr}.torch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prG5whv_wQtH",
        "outputId": "a12e228d-00a4-4127-cf9b-da1310865dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ) Loss = 0.81939 -- Accuracy = 0.09765\n",
            "Saving 0.torch\n",
            "10 ) Loss = 0.74898 -- Accuracy = 0.31476\n",
            "Saving 10.torch\n",
            "20 ) Loss = 0.71071 -- Accuracy = 0.46811\n",
            "Saving 20.torch\n",
            "30 ) Loss = 0.65425 -- Accuracy = 0.75519\n",
            "Saving 30.torch\n",
            "40 ) Loss = 0.61035 -- Accuracy = 0.83979\n",
            "Saving 40.torch\n",
            "50 ) Loss = 0.58145 -- Accuracy = 0.95638\n",
            "Saving 50.torch\n",
            "60 ) Loss = 0.55059 -- Accuracy = 0.96035\n",
            "Saving 60.torch\n",
            "70 ) Loss = 0.51686 -- Accuracy = 0.96542\n",
            "Saving 70.torch\n",
            "80 ) Loss = 0.49547 -- Accuracy = 0.99684\n",
            "Saving 80.torch\n",
            "90 ) Loss = 0.48958 -- Accuracy = 0.98169\n",
            "Saving 90.torch\n",
            "100 ) Loss = 0.4418 -- Accuracy = 0.99707\n",
            "Saving 100.torch\n",
            "110 ) Loss = 0.4357 -- Accuracy = 0.99865\n",
            "Saving 110.torch\n",
            "120 ) Loss = 0.43033 -- Accuracy = 0.99862\n",
            "Saving 120.torch\n",
            "130 ) Loss = 0.41902 -- Accuracy = 0.99905\n",
            "Saving 130.torch\n",
            "140 ) Loss = 0.39977 -- Accuracy = 0.99672\n",
            "Saving 140.torch\n",
            "150 ) Loss = 0.39665 -- Accuracy = 0.99874\n",
            "Saving 150.torch\n",
            "160 ) Loss = 0.36793 -- Accuracy = 0.99811\n",
            "Saving 160.torch\n",
            "170 ) Loss = 0.35273 -- Accuracy = 0.99863\n",
            "Saving 170.torch\n",
            "180 ) Loss = 0.36375 -- Accuracy = 0.99789\n",
            "Saving 180.torch\n",
            "190 ) Loss = 0.35613 -- Accuracy = 0.99866\n",
            "Saving 190.torch\n"
          ]
        }
      ]
    }
  ]
}