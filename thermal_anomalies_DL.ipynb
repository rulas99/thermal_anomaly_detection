{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thermal_anomalies_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rulas99/thermal_anomaly_detection/blob/main/thermal_anomalies_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Segmentation [source](https://towardsdatascience.com/train-neural-net-for-semantic-segmentation-with-pytorch-in-50-lines-of-code-830c71a6544f)"
      ],
      "metadata": {
        "id": "aekdsIGqHvuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "alFBr4KgC-gb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models.segmentation\n",
        "import torch\n",
        "import torchvision.transforms as tf\n",
        "\n",
        "from pandas import read_csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "164j9ty19a8I",
        "outputId": "9cc0150b-707c-42e7-d3a6-e4ef5dafba74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 1e-5\n",
        "width = 500\n",
        "height = 500 # image width and height\n",
        "batchSize = 7"
      ],
      "metadata": {
        "id": "7N5s4KO5wC0h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelPath = \"/content/drive/MyDrive/Inge/segment_sentinel2_hotspot/Models_2/model_2030_v1.torch\" "
      ],
      "metadata": {
        "id": "TPANbxYl-J67"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True) # Load net\n",
        "\n",
        "Net.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 2 classes\n",
        "\n",
        "Net = Net.to(device) # Set net to GPU or CPU\n",
        "\n",
        "# Net.load_state_dict(torch.load(modelPath)) # Load trained model\n",
        "\n",
        "optimizer = torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
        "\n",
        "weights = torch.FloatTensor([0.01,1]).cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss(weight = weights) # Set loss function"
      ],
      "metadata": {
        "id": "Lx6dEFuLwLkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = read_csv(\"/content/drive/MyDrive/segment_sentinel2_hotspot/train_2.csv\")\n",
        "train = read_csv(\"/content/drive/MyDrive/segment_sentinel2_hotspot/train_raw.csv\")"
      ],
      "metadata": {
        "id": "PRKw-z48wAVu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train2[~train2.label_path.isin(train1.label_path)]"
      ],
      "metadata": {
        "id": "Emf0zsk1_M4s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)),tf.ToTensor(),])\n",
        "                         #tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "                         \n",
        "transformAnn=tf.Compose([tf.ToPILImage(),tf.Resize((height,width),tf.InterpolationMode.NEAREST),tf.ToTensor()])"
      ],
      "metadata": {
        "id": "cr6fGoMHAgsQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReadRandomImage(): # First lets load random image and  the corresponding annotation\n",
        "    idx=np.random.randint(0,len(train)) # Select random image\n",
        "    thermal = cv2.imread(train.ori_path.iloc[idx])\n",
        "    masked =  cv2.imread(train.label_path.iloc[idx],0)\n",
        "    AnnMap = np.zeros(thermal.shape[0:2],np.float32)\n",
        "    if masked is not None:  AnnMap[ masked == 255 ] = 1\n",
        "    Img=transformImg(thermal)\n",
        "    AnnMap=transformAnn(AnnMap)\n",
        "    return Img,AnnMap\n",
        "\n",
        "\n",
        "def LoadBatch(): # Load batch of images\n",
        "    images = torch.zeros([batchSize,3,height,width])\n",
        "    ann = torch.zeros([batchSize, height, width])\n",
        "    for i in range(batchSize):\n",
        "        images[i],ann[i]=ReadRandomImage()\n",
        "    return images, ann"
      ],
      "metadata": {
        "id": "yRdqSGrxwJMk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for itr in range(5000): # Training loop\n",
        "   images,ann=LoadBatch() # Load taining batch\n",
        "   images=torch.autograd.Variable(images,requires_grad=False).to(device) # Load image\n",
        "   ann = torch.autograd.Variable(ann, requires_grad=False).to(device) # Load annotation\n",
        "   Pred=Net(images)['out'] # make prediction\n",
        "   optimizer.zero_grad()\n",
        "   Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss\n",
        "   Loss.backward() # Backpropagate loss\n",
        "   optimizer.step() # Apply gradient descent change to weight\n",
        "\n",
        "   loss = round(float(Loss.data.cpu()),3)\n",
        "\n",
        "   if itr % 250 == 0 or loss<0.06: #Save model weight once every 100 steps permenant file\n",
        "        #accG = [(torch.argmax(Pred[i], 0).cpu().detach().numpy() == ann[i].cpu().detach().numpy()).sum()/(width*height) for i in range(batchSize)]\n",
        "        #accM = round(sum(accG)/batchSize,5)\n",
        "        print(itr,f\") Loss = {loss}\")\n",
        "        #print(f\"Saving model_{itr}.torch\")\n",
        "        torch.save(Net.state_dict(),   f'/content/drive/MyDrive/segment_sentinel2_hotspot/Models_2/model_{itr}_v2.torch')\n",
        "\n",
        "        if loss<0.06:\n",
        "          break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prG5whv_wQtH",
        "outputId": "f5c27b3e-9388-4f89-a360-1dcc6333e51c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ) Loss = 0.668\n",
            "250 ) Loss = 0.212\n",
            "500 ) Loss = 0.214\n",
            "598 ) Loss = 0.053\n"
          ]
        }
      ]
    }
  ]
}